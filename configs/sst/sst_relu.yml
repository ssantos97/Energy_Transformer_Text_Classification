default_root_dir: 
- experiments/sst/relu

save_rationales: 
- True       # rationales will be saved in a .csv in the default_root_dir as `rationales-{timestamp}.csv`
save_tokenizer: 
- True        # the tokenizer will be pickled and store in the checkpoint dir as `tokenizer.pickle`
save_label_encoder:
- True    # the label encoder will be pickled and store in the checkpoint dir as `label_encoder.pickle`
gpu_log:
- False              # whether to use the gpu callback to see gpu information in the logger

# data
dm: 
- sst                     # data module name (see docs for more options)
batch_size:
- 32                  # minibatch size
num_workers:
- 4                  # number of workers used for data loading (0 means that only a single core will be used)
vocab_min_occurrences: 
- 1       # frequency for a token to be added to the vocabulary

# early stopping
monitor: 
- val_f1score     # quantity to be monitored
monitor_mode: 
- max         # whether to see if monitored metric has stopped decreasing (min) or increasing (max)
monitor_patience: 
- 20         # number of epochs to wait for early stopping


optimizer: 
- adam
lr: 
- 0.001
- 0.0001
weight_decay: 
- 0.001
betas: 
- [0.9, 0.999]
amsgrad:
- False
momentum: 
- 0.0
dampening:
- 0.0
nesterov:
- False
alpha:
- 0.99   # for rmsprop
centered:
- False  # for rmsprop
lambd: 
- 0.0001  # for asgd
t0: 
- 1000000.0  # for asgd

  # model: lr scheduler
scheduler: 
- plateau
milestones:
- [25, 50, 75]
lr_decay:
- 0.97
- 0.94  # a.k.a gamma
patience:
- 5
cooldown:
- 0
threshold:
- 0.0001
min_lr:
- 0
eta_min:
- 0.0000001
T_0:
- 2800
T_mult:
- 2

# model: architecture
emb_type: 
- glove
emb_path:
- 840B 

# trainer (will be passed to pytorch-lightning's Trainer object)
# see the complete list here: https://pytorch-lightning.readthedocs.io/en/stable/trainer.html#trainer-flags
gpus:
- 1
gradient_clip_val: 
- 5.0
min_epochs:
- 45
max_epochs:
- 60
last_epoch:
- False

#Model:
out_dim:
- 128
- 256
tkn_dim:
- 300
qk_dim:
- 32
- 64
nheads: 
- 8
- 16
hn_mult: 
- 4
- 2
attn_beta: 
- None
attn_bias: 
- None
hn_bias: 
- None
hn_fn: 
- relu
time_steps: 
- 12
blocks: 
- 1
alpha: 
- 1 
use_cls:
- True